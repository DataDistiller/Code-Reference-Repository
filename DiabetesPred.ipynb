{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getageweightgroups():\n",
    "    \n",
    "    ageweightgroups = {}\n",
    "    \n",
    "    agebins = ['['+str(x)+'-'+str(x+10)+')' for x in np.arange(0,100,10)] \n",
    "    agebinvals = np.arange(len(agebins))\n",
    "    ageweightgroups.update({'age' : dict(zip(agebins,agebinvals))})\n",
    "    \n",
    "    weightbins = ['['+str(x)+'-'+str(x+25)+')' for x in np.arange(0,200,25)] \n",
    "    weightbinvals = np.arange(len(weightbins))\n",
    "\n",
    "    ageweightgroups.update({'weight' : dict(zip(weightbins,weightbinvals))})\n",
    "    \n",
    "    return ageweightgroups\n",
    "\n",
    "# function to cleanup / replace bad data and change data types of known columns\n",
    "\n",
    "def standardize(df):\n",
    "        \n",
    "    # replace ? with NaN\n",
    "    df1 = df.copy()\n",
    "\n",
    "    # Make all ? as Nan\n",
    "    df1 = df1.replace('?', np.NaN)\n",
    "\n",
    "    # make all char/obj type columns upper and strip spaces if any\n",
    "    for catcol in df1.select_dtypes(include=['object']).columns:\n",
    "        df1[catcol] = df1[catcol].str.strip()\n",
    "        df1[catcol] = df1[catcol].str.upper()\n",
    "\n",
    "    # remove values which have chars in tel9 / tel10 /tel11\n",
    "    for col in ['tel_9','tel_10','tel_11']:\n",
    "        df1[col] = df1[col].replace(to_replace ='[\\D]', value = np.NaN, regex = True) \n",
    "        df1[col] = df1[col].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "    ageweightgroups = getageweightgroups()\n",
    "    df1['weight'] = df1['weight'].apply(lambda x:ageweightgroups['weight'][x] if x in ageweightgroups['weight'].keys() else None )\n",
    "    df1['age']  = df1['age'].apply(lambda x:ageweightgroups['age'][x] if x in ageweightgroups['age'].keys() else None )\n",
    " \n",
    "    return df1\n",
    "\n",
    "# function to predict a missing value from other records with non-null values\n",
    "# the variable y_col is predicted based on other records of the columns set x_cols\n",
    "def fillna_with_predictions(df,x_cols,y_col):\n",
    "\n",
    "    d = df[x_cols + [y_col]]\n",
    "    # create one hot encoded features for the X columns\n",
    "    dum = pd.get_dummies(d, columns = x_cols)\n",
    "    \n",
    "    # get the data which has a non-null value of the target var y_col\n",
    "    masknotnull = dum[y_col].notnull()\n",
    "    # build features X and y (considering all data)\n",
    "    X = dum[masknotnull].loc[:,~dum.columns.isin([y_col])]\n",
    "    y = dum[masknotnull][y_col]\n",
    "\n",
    "    # fit a RF classifier\n",
    "    rf = RandomForestClassifier(random_state = 1, n_estimators = 200)\n",
    "    rf.fit(X,y)\n",
    "\n",
    "    # predict the data for the rows which has missing y_col\n",
    "    X = dum[~masknotnull].loc[:,~dum.columns.isin([y_col])]\n",
    "    pred = rf.predict(X)\n",
    "\n",
    "    # update the missing y_col data\n",
    "    dum.loc[~masknotnull,y_col] = pred.reshape(len(pred),1)\n",
    "\n",
    "    #return them missing column\n",
    "    return dum[y_col]\n",
    "\n",
    "\n",
    "# function to do a fillna with a specific type of strategy\n",
    "def fillna_with_values(df,cols,how):\n",
    "\n",
    "    df1 = df.copy()\n",
    "    # impute with string \"UNKNOWN\"\n",
    "    if how == 'UNKNOWN':\n",
    "        df1[cols] = df1[cols].fillna('UNKNOWN')\n",
    "    # Impute with median of the column\n",
    "    elif how == 'MEDIAN':\n",
    "        for c in cols:\n",
    "            df1[c] = df1[c].fillna(df1[c].median())\n",
    "    # Impute with median of the column\n",
    "    elif how == 'MODE':\n",
    "        for c in cols:\n",
    "            df1[c] = df1[c].fillna(df1[c].mode().values[0])\n",
    "    return df1\n",
    "\n",
    "\n",
    "# function to impute missing data \n",
    "# calls the above two functions based on specific case of the column\n",
    "def impute_data(df):\n",
    "\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    df1 = fillna_with_values(df1,['race'],'UNKNOWN')\n",
    "    df1 = fillna_with_values(df1,['tel_9', 'tel_10', 'tel_11'],'MEDIAN')\n",
    "    \n",
    "    df1['weight'] = fillna_with_predictions(df = df1, x_cols = ['race', 'gender', 'age'] , y_col = 'weight')\n",
    "    df1['tel_1'] = fillna_with_predictions(df = df1, x_cols = ['admission_type_id','discharge_disposition_id', 'admission_source_id'] , \n",
    "                                  y_col = 'tel_1')\n",
    "    df1['tel_2'] = fillna_with_predictions(df = df1, x_cols = ['admission_type_id','discharge_disposition_id', 'admission_source_id'] , \n",
    "                                  y_col = 'tel_2')\n",
    "\n",
    "    return df1\n",
    "\n",
    "\n",
    "# function to drop unwanted columns and change data types after the standardizations are done\n",
    "def cleanup_cols(df):\n",
    "    \n",
    "    df1 = df.copy()    \n",
    "\n",
    "    # these columns are either 'constant' columns with only one value or id fields \n",
    "    df1 = df1.drop(columns = ['encounter_id','patient_id','tel_18','tel_20','tel_23','tel_28', 'tel_29',\n",
    "                              'tel_30', 'tel_41','tel_45','tel_46','tel_47'])  \n",
    "\n",
    "    return df1\n",
    "\n",
    "# create a custom Encoder class to handle new unseen values as UNKNOWN \n",
    "# this is an extension of the sklearn Label Encoder class\n",
    "class MyLabelEncoder():\n",
    "    \n",
    "    # initialize encoder with base constructor of LabelEncoder\n",
    "    def __init__(self):\n",
    "        self.encoder = LabelEncoder()\n",
    "\n",
    "    def fit(self, values):\n",
    "        # add a category \"UNKNOWN\"\n",
    "        # this will be used as the category when an \"unseen\" value shows up in the data set\n",
    "        self.encoder = self.encoder.fit(list(values) + ['UNKNOWN'])\n",
    "        self.classes_ = self.encoder.classes_\n",
    "\n",
    "    def transform(self,values):\n",
    "\n",
    "        values = list(values)\n",
    "        # for each item in new values\n",
    "        for item in np.unique(values):\n",
    "            # if the item is not in the encoder classes (new unseen value   ) mark it as 'UNKNOWN'\n",
    "            if item not in self.classes_:\n",
    "                values = ['UNKNOWN' if x == item else x for x in values]\n",
    "\n",
    "        return self.encoder.transform(values)\n",
    "\n",
    "# Function to build encoder list for columns\n",
    "# Generate a dictionary of encoders to be used for test set\n",
    "def generate_encoder(df):\n",
    "\n",
    "    encdict = {}\n",
    "\n",
    "    for col in df.select_dtypes(include = ['object']).columns:\n",
    "        enc = MyLabelEncoder()\n",
    "        enc.fit(df[col])\n",
    "        encdict.update({col : enc})\n",
    "\n",
    "    return encdict \n",
    "\n",
    "\n",
    "# Encode the data using the passed in encoders\n",
    "\n",
    "def encode_data(df,encoders):\n",
    "\n",
    "    df1 = df.copy()\n",
    "\n",
    "    for col in df1.select_dtypes(include = ['object']).columns:\n",
    "        df1[col]  = encoders[col].transform(df1[col])\n",
    "\n",
    "    return df1\n",
    "\n",
    "# function to call all the above function and prepare data for modeling\n",
    "def clean_data(df):\n",
    "\n",
    "    df1 = cleanup_cols(df)\n",
    "    assert len(df1) == len(df), \"data loss observed after cleanupcols\"\n",
    "\n",
    "    df1 = standardize(df1)\n",
    "    assert len(df1) == len(df), \"data loss observed after standardization\"\n",
    "\n",
    "    df1 = impute_data(df1)\n",
    "    assert len(df1) == len(df), \"data loss observed after impute\"\n",
    "\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_model(df):\n",
    "    df1 = df.copy()\n",
    "    encoders = generate_encoder(df1)\n",
    "    train_clean = encode_data(df1,encoders)\n",
    "    test_clean = encode_data(df1,encoders)\n",
    "    assert train_clean.isnull().sum().sum() == 0, \"missing data still exists in train\"\n",
    "    assert test_clean.isnull().sum().sum() == 0, \"missing data still exists in test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df):\n",
    "    df1 = df.copy()\n",
    "    train_clean = encode_data(df1,encoders)\n",
    "    test_clean = encode_data(df1,encoders)\n",
    "    assert train_clean.isnull().sum().sum() == 0, \"missing data still exists in train\"\n",
    "    assert test_clean.isnull().sum().sum() == 0, \"missing data still exists in test\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in train set = 14696 and test set = 7970 \n",
      "Number of rows in clean train set = 14696 and test set = 7970 \n",
      "Number of rows in encoded train set = 14696 and test set = 7970 \n",
      "Model test score :0.9989112683723462\n",
      "Confusion Matrix:\n",
      "\n",
      "\n",
      "[[ 371    1]\n",
      " [   3 3299]]\n",
      "Classification report:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       372\n",
      "           1       1.00      1.00      1.00      3302\n",
      "\n",
      "    accuracy                           1.00      3674\n",
      "   macro avg       1.00      1.00      1.00      3674\n",
      "weighted avg       1.00      1.00      1.00      3674\n",
      "\n",
      "1    4555\n",
      "0    3415\n",
      "Name: diabetesMed, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "# startig main section\n",
    "#####################################################\n",
    "\n",
    "# read train and test files\n",
    "train = pd.read_csv(\"patientdata\\\\train.csv\", encoding = 'utf-8')\n",
    "train_clean = clean_data(train)\n",
    "# 14696\n",
    "\n",
    "test = pd.read_csv(\"patientdata\\\\test.csv\", encoding = 'utf-8')\n",
    "test_clean = clean_data(test)\n",
    "# 7970\n",
    "\n",
    "print(\"Number of rows in train set = {} and test set = {} \".format(len(train), len(test)))\n",
    "print(\"Number of rows in clean train set = {} and test set = {} \".format(len(train_clean), len(test_clean)))\n",
    "\n",
    "encoders = generate_encoder(train_clean)\n",
    "train_clean = encode_data(train_clean,encoders)\n",
    "test_clean = encode_data(test_clean,encoders)\n",
    "\n",
    "assert train_clean.isnull().sum().sum() == 0, \"missing data still exists in train\"\n",
    "assert test_clean.isnull().sum().sum() == 0, \"missing data still exists in test\"\n",
    "\n",
    "print(\"Number of rows in encoded train set = {} and test set = {} \".format(len(train_clean), len(test_clean)))\n",
    "\n",
    "assert len(train_clean.select_dtypes(include = ['object']).columns) == 0, \"non-numerical data exists\"\n",
    "\n",
    "# #############################################################################\n",
    "\n",
    "# Get model with train data\n",
    "# generate features\n",
    "\n",
    "# pos = train_clean[train_clean['diabetesMed'] == 1].sample(5000)\n",
    "# neg = train_clean[train_clean['diabetesMed'] == 0]\n",
    "# train_clean = pd.concat([pos,neg])\n",
    "\n",
    "X = train_clean.loc[:,~train_clean.columns.isin(['diabetesMed'])]\n",
    "y = train_clean['diabetesMed']\n",
    "\n",
    "# Split Train Test Sets, ensure stratify to include positive and negative examples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25, random_state = 10, stratify = y)\n",
    "\n",
    "# Hyperparameter tuning is done as a separate excercise\n",
    "# in this platform, we dont have enough memory to run RandomizedSearchCV\n",
    "# and do a hyperparameter grid search\n",
    "rfc = RandomForestClassifier(random_state = 1, n_estimators = 500)\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred = rfc.predict(X_test)\n",
    "\n",
    "print(\"Model test score :{}\".format(rfc.score(X_test,y_test)))\n",
    "print(\"Confusion Matrix:\\n\\n\")\n",
    "print(confusion_matrix(y_test,y_test_pred))\n",
    "print(\"Classification report:\\n\\n\")\n",
    "print(classification_report(y_test,y_test_pred))\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# # Predict with test data\n",
    "\n",
    "X = test_clean.loc[:,:]\n",
    "pred = rfc.predict(X)\n",
    "test['diabetesMed'] = pred.reshape(len(pred),1)\n",
    "\n",
    "# get cols required for submission\n",
    "submission = test[['encounter_id', 'diabetesMed']]\n",
    "\n",
    "print(submission['diabetesMed'].value_counts())\n",
    "# Submit to csv\n",
    "submission.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
